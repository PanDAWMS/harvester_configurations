#!/bin/sh
#DPB #SBATCH -q debug
#DPB #SBATCH --time 0:30:00
#DPB #SBATCH -q premium
#DPB #SBATCH --time 2:00:00
#DPB #SBATCH -q premium
#SBATCH -q regular
#DPB #SBATCH -q low
#SBATCH --time 9:00:00
#SBATCH --module=cvmfs
#DPB #SBATCH -q flex
##SBATCH --time-min=02:00:00
#DPB #SBATCH --time 4:00:00
#SBATCH -A m2616
##SBATCH -A m3651
#SBATCH -L SCRATCH,project
#SBATCH -C knl,quad,cache  
#SBATCH --cpus-per-task 136
#SBATCH -N {nNode}

# change to group m2616 (ERCAP group)
echo
echo [$SECONDS] "change to group m2616 (ERCAP group) via command : newgrp m2616"
newgrp m2616
echo

export MPICH_GNI_VC_MSG_PROTOCOL=MSGQ
export KMP_AFFINITY="granularity=fine,compact,1,0"
nTasks=$SLURM_JOB_NUM_NODES

#export container_setup=/usr/atlas/release_setup.sh

export PANDA_QUEUE=NERSC_Cori_p2_mcore

export HARVESTER_DIR=$PANDA_HOME  # PANDA_HOME is defined in etc/sysconfig/panda_harvester
export HARVESTER_WORKER_ID={workerID}
export HARVESTER_ACCESS_POINT={accessPoint}
export HARVESTER_NNODE={nNode}
export HARVESTER_NTASKS=$nTasks

#set variable to define HARVESTER running mode (mapType) for this worker
#  possible choices OneToOne, OneToMany, ManyToOne
export HARVESTER_MAPTYPE=ManyToOne


#source $HARVESTER_DIR/setup.sh
#source $HARVESTER_DIR/etc/sysconfig/panda_harvester

export pilot_wrapper_file=$HARVESTER_DIR/etc/panda/runpilot2-wrapper.sh
export pilot_tar_file=$HARVESTER_DIR/etc/panda/pilot2.tar.gz

export pilot_cric_pandaqueues_file=$HARVESTER_DIR/cric_pandaqueues.json

export pilot_schedconfig_JSON_file=$HARVESTER_DIR/NERSC_Cori_p2_mcore_agis_schedconf.json   # extra ?
export pilot_queuedata_file=$HARVESTER_DIR/NERSC_Cori_p2_mcore_queuedata.json
export pilot_ddmendpoints_file=$HARVESTER_DIR/cric_ddmendpoints.json

#export HARVESTER_CONTAINER_IMAGE=custom:atlas_athena_21.0.15_DBRelease-31.8.1:latest
# variables create the string to setup container release path including fixing PYTHONPATH and LD_LIBRARY_PATH for yampl
#export HARVESTER_CONTAINER_RELEASE_SETUP_FILE="/usr/atlas/release_setup.sh"
#export HARVESTER_LD_LIBRARY_PATH=$LD_LIBRARY_PATH
#export HARVESTER_PYTHONPATH=$PYTHONPATH 

# set ATLAS Local Root Base options
export ALRB_CONT_OPTS="$ALRB_CONT_OPTS -v"
export ALRB_CONT_SWTYPE=shifter

#Athena settings for AthenaMP
export ATHENA_PROC_NUMBER_JOB=136
export ATHENA_PROC_NUMBER=136
export ATHENA_CORE_NUMBER=136

# get the PanDA Job IDs from worker_pandaids.json 
PANDA_JOB_IDS=( "$(python -c "import json;pids = json.load(open('worker_pandaids.json'));print(' '.join(str(x) for x in pids))")" )

export X509_USER_PROXY=$HARVESTER_DIR/var/lib/x509up_vomsproxy

# record the various HARVESTER envars
echo
echo [$SECONDS] "Harvester Top level directory - "$HARVESTER_DIR
echo [$SECONDS] "Harvester accessPoint - "$HARVESTER_ACCESS_POINT
echo [$SECONDS] "Harvester Worker ID - "$HARVESTER_WORKER_ID
echo [$SECONDS] "Harvester workflow (MAPTYPE) - "$HARVESTER_MAPTYPE
echo [$SECONDS] "Harvester accessPoint - "$HARVESTER_ACCESS_POINT
echo [$SECONDS] "pilot wrapper file - "$pilot_wrapper_file
echo [$SECONDS] "Pilot tar file - "$pilot_tar_file
echo [$SECONDS] "Container IMAGE_BASE - "$IMAGE_BASE
echo [$SECONDS] "command to setup release in container - "$HARVESTER_CONTAINER_RELEASE_SETUP_FILE
echo [$SECONDS] PANDA_JOB_IDS: $PANDA_JOB_IDS
echo [$SECONDS] "Number of Nodes to use - "$HARVESTER_NNODE
echo [$SECONDS] "Number of tasks for srun - "$HARVESTER_NTASKS 
echo [$SECONDS] "ATHENA_PROC_NUMBER - "$ATHENA_PROC_NUMBER
echo

echo [$SECONDS] TMPDIR = $TMPDIR
echo [$SECONDS] unset TMPDIR
unset TMPDIR
echo [$SECONDS] TMPDIR = $TMPDIR

echo [$SECONDS] ATLAS_LOCAL_ROOT_BASE = $ATLAS_LOCAL_ROOT_BASE
#echo [$SECONDS] unset ATLAS_LOCAL_ROOT_BASE
#unset ATLAS_LOCAL_ROOT_BASE
echo [$SECONDS] ATLAS_LOCAL_ROOT_BASE = $ATLAS_LOCAL_ROOT_BASE
echo [$SECONDS] ALRB_CONT_SWTYPE = $ALRB_CONT_SWTYPE
echo [$SECONDS] "ALRB_CONT_OPTS - "$ALRB_CONT_OPTS

# In OneToOne running Harvester expects fdiles in $HARVESTER_ACCESS_POINT
# In OneToMany running (aka Jumbo jobs) Harvester expects fdiles in $HARVESTER_ACCESS_POINT
#    Note in jumbo job running pilot wrapper will ensure each pilot runs in a different directory

# Loop over PanDA ID's

for PANDA_JOB_ID in $PANDA_JOB_IDS;  do

    echo [$SECONDS] "PanDA Job ID - "$PANDA_JOB_ID
    
    # test if running in ManyToOne mode and set working directory accordingly
    export HARVESTER_WORKDIR=$HARVESTER_ACCESS_POINT
    if [[ "$HARVESTER_MAPTYPE" = "ManyToOne" ]] ; then
	export HARVESTER_WORKDIR=$HARVESTER_ACCESS_POINT/$PANDA_JOB_ID
    fi    
    echo [$SECONDS] "setting Harvester Work Directory to - "$HARVESTER_WORKDIR

    # move into job directory
    echo [$SECONDS] "Changing to $HARVESTER_WORKDIR "
    cd $HARVESTER_WORKDIR
    
    #copy pilot wrapper and panda pilot tarball to working directory
    echo [$SECONDS] copy $pilot_wrapper_file to $HARVESTER_WORKDIR
    cp -v $pilot_wrapper_file $HARVESTER_WORKDIR
    echo [$SECONDS] copy $pilot_tar_file to $HARVESTER_WORKDIR
    cp -v $pilot_tar_file $HARVESTER_WORKDIR
    
    # test for json files used by pilot's infoservice
    if [ ! -f $pilot_cric_pandaqueues_file ] ; then
        echo [$SECONDS] Warning the file - "$pilot_cric_pandaqueues_file" does not exist
    else
        cp -v $pilot_cric_pandaqueues_file $HARVESTER_WORKDIR/cric_pandaqueues.json
    fi

    if [ ! -f $pilot_queuedata_file ] ; then
        echo [$SECONDS] Warning the file - "$pilot_queuedata_file" does not exist
    else
        cp -v $pilot_queuedata_file $HARVESTER_WORKDIR/queuedata.json 
    fi   

    if [ ! -f $pilot_ddmendpoints_file ] ; then
        echo [$SECONDS] Warning the file - "$pilot_ddmendpoints_file" does not exist
    else
        cp -v $pilot_ddmendpoints_file $HARVESTER_WORKDIR/cric_ddmendpoints.json 
    fi   

    #DPB debugging
    #env | sort
    #DPB debugging

    # In ManyToOne running or OnetoOne running want one launch on srun per node - per PanDAid
    if [[ "$HARVESTER_MAPTYPE" = "OneToMany" ]] ; then    #AKA Jumbo jobs
        # now start things up
	echo [$SECONDS] "Launching srun command from : " $PWD

	echo [$SECONDS] srun -n $nTasks  -o PandaID-$PANDA_JOB_ID-%N-%j-%t.log \
	     shifter /bin/bash ./runpilot2-wrapper.sh -s $PANDA_QUEUE -r $PANDA_QUEUE \
	     -q $PANDA_QUEUE -j managed -i PR -t --mute --harvester SLURM_NODEID \
	     --harvester_workflow $HARVESTER_MAPTYPE \
	     --container -w generic --pilot-user=atlashpc \
	     -d --harvester-submit-mode=PUSH \
	     --harvester-workdir=$HARVESTER_WORKDIR \
	     --allow-same-user=False --resource-type MCORE -z --cleanup True

	srun -n $nTasks  -o PandaID-$PANDA_JOB_ID-%N-%j-%t.log \
	     shifter /bin/bash ./runpilot2-wrapper.sh -s $PANDA_QUEUE -r $PANDA_QUEUE \
	     -q $PANDA_QUEUE -j managed -i PR -t --mute --harvester SLURM_NODEID \
	     --harvester_workflow $HARVESTER_MAPTYPE \
	     --container -w generic --pilot-user=atlashpc \
	     -d --harvester-submit-mode=PUSH \
	     --harvester-workdir=$HARVESTER_WORKDIR \
	     --allow-same-user=False --resource-type MCORE -z --cleanup True &
    else
        # now start things up
	echo [$SECONDS] "Launching srun command from : " $PWD

	echo [$SECONDS] srun -n 1 -N 1 -o PandaID-$PANDA_JOB_ID-%N-%j-%t.log \
            /bin/bash ./runpilot2-wrapper.sh -3 --localpy -s $PANDA_QUEUE -r $PANDA_QUEUE -q $PANDA_QUEUE \
                      -j managed -i PR -t --mute --harvester SLURM_NODEID --harvester_workflow $HARVESTER_MAPTYPE \
                      --container -w generic --pilot-user=atlas --hpc-resource cori --use-https False -d --harvester-submit-mode=PUSH \
                      --harvester-workdir=$HARVESTER_WORKDIR --allow-same-user=False --resource-type MCORE --cleanup True -z

        srun -n 1 -N 1 -o PandaID-$PANDA_JOB_ID-%N-%j-%t.log \
            /bin/bash ./runpilot2-wrapper.sh -3 --localpy -s $PANDA_QUEUE -r $PANDA_QUEUE -q $PANDA_QUEUE \
                      -j managed -i PR -t --mute --harvester SLURM_NODEID --harvester_workflow $HARVESTER_MAPTYPE \
                      --container -w generic --pilot-user=atlas --hpc-resource cori --use-https False -d --harvester-submit-mode=PUSH \
                      --harvester-workdir=$HARVESTER_WORKDIR --allow-same-user=False --resource-type MCORE -z --cleanup True &

    fi
    

done

wait
