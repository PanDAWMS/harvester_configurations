executable = {executableFile}
arguments = -s {computingSite} -r {computingSite} -q {pandaQueueName} -j {prodSourceLabel} -i {pilotType} {pilotPythonOption} -w generic --pilot-user ATLAS --url https://pandaserver.cern.ch --harvester-submit-mode PUSH {pilotResourceTypeOption} --pilotversion {pilotVersion} {pilotUrlOption}
initialdir = {accessPoint}
universe = grid
log = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).log
output = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).out
error = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).err
transfer_executable = True
x509userproxy = {x509UserProxy}
environment = "PANDA_JSID=harvester-{harvesterID} HARVESTER_ID={harvesterID} HARVESTER_WORKER_ID={workerID} GTAG={gtag} APFMON=http://apfmon.lancs.ac.uk/api APFFID={harvesterID} APFCID=$(Cluster).$(Process) PANDA_AUTH_ORIGIN=atlas.pilot PANDA_AUTH_TOKEN={pandaTokenFilename} PANDA_AUTH_TOKEN_KEY={pandaTokenKeyFilename}"
+harvesterID = "{harvesterID}"
#transfer_input_files = pandaID.out
transfer_input_files = pandaJobData.out,{pandaTokenPath},{pandaTokenKeyPath}
DelegateJobGSICredentialsLifetime = 0

grid_resource = condor {ceHostname} {ceEndpoint}
+remote_jobuniverse = 5
+remote_ShouldTransferFiles = "YES"
+remote_WhenToTransferOutput = "ON_EXIT_OR_EVICT"
+remote_TransferOutput = ""
+remote_RequestCpus   = quantize({nCoreTotal}, 1)
+remote_RequestMemory = quantize({requestRam}, 128)
+remote_JobMaxVacateTime = {requestWalltime}

+remote_Requirements = ( TARGET.Cpus >= RequestCpus ) && ( TARGET.Memory >= RequestMemory )
+remote_PeriodicRemove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)

+ioIntensity = {ioIntensity}
+sdfPath = "{sdfPath}"
queue 1
